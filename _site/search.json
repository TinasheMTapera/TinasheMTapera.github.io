[
  {
    "objectID": "posts/listing-test/01-test_post.html",
    "href": "posts/listing-test/01-test_post.html",
    "title": "test1",
    "section": "",
    "text": "This is a post"
  },
  {
    "objectID": "posts/listing-test/listing-test.html",
    "href": "posts/listing-test/listing-test.html",
    "title": "another listing",
    "section": "",
    "text": "test1\n\n\n\n\n\n\nTinashe M. Tapera\n\n\nJul 29, 2022\n\n\n\n\n\n\n\n\n\n\n\ntest2\n\n\n\n\n\n\nTinashe M. Tapera\n\n\nAug 1, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/listing-test/02-test_post.html",
    "href": "posts/listing-test/02-test_post.html",
    "title": "test2",
    "section": "",
    "text": "This is a second test post"
  },
  {
    "objectID": "posts/01_test-quarto/01_test-quarto.html",
    "href": "posts/01_test-quarto/01_test-quarto.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/listing-legacy/2018-09-22-encourageme-a-shiny-app-for-encouragements.html",
    "href": "posts/listing-legacy/2018-09-22-encourageme-a-shiny-app-for-encouragements.html",
    "title": "EncourageMe: A Shiny App for Encouragement",
    "section": "",
    "text": "Ironically, I only learned exactly how effective they could be when someone was kind enough to use them with me. Earlier this year, a close friend1 went through the trouble of handwriting a bunch of encouragements for me when I was struggling to keep up with thesis work, research, classes, depression/anxiety, job search, and everything else that was happening in life at the time. After pulling myself out of that difficult period, I decided to return the favour by building an RShiny app that she (or anybody else) could use if they ever needed some encouragement.\nThe app is hosted by R Studio at shinyapps.io, but I’ve embedded it below:\n\n\n\n\n\n\nTech Specs\nThe app itself is very simple: on launch, it reads in a file of quotes I hand picked, and simply samples one every time you hit the action button.\nWeb programming with RShiny gets more and more intersting every time you do it. I highly recommend it to anybody who has experience with R and wants to grow. RShiny allows you to write all of your basic application/dashboard functionality in R, and then when you want to spice it up, you can get your hands dirty (slowly) with HTML and CSS pretty seamlessly. In this app, I made aesthetic modifications (like the fonts and footer), but with web programming your only limitations are your imagination, and determination to make it work! You can examine the code on Github.\n\n\n\n\n\nFootnotes\n\n\nThis “friend” is now my wife. Go figure!↩︎"
  },
  {
    "objectID": "posts/listing-legacy/2018-10-23-data-tip-processing-checkbox-factors.html",
    "href": "posts/listing-legacy/2018-10-23-data-tip-processing-checkbox-factors.html",
    "title": "Data Tip: Processing Checkbox-Factors",
    "section": "",
    "text": "Let’s take a look at what this might look like, using the Star Wars dataset in the most recent version of dplyr:\n\ndplyr::starwars\n\n# A tibble: 87 × 14\n   name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n   <chr>        <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n 1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n 2 C-3PO          167    75 <NA>    gold    yellow    112   none  mascu… Tatooi…\n 3 R2-D2           96    32 <NA>    white,… red        33   none  mascu… Naboo  \n 4 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n 5 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n 6 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n 7 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n 8 R5-D4           97    32 <NA>    white,… red        NA   none  mascu… Tatooi…\n 9 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n10 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n# … with 77 more rows, 4 more variables: species <chr>, films <list>,\n#   vehicles <list>, starships <list>, and abbreviated variable names\n#   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nsw = dplyr::starwars\n\nThe variable skin_color is a good candidate for us to learn how to process this type of response. In this variable, Obi-Wan’s hair colour is listed as “auburn, white”, which would be like checking two boxes in a survey. In order to get, for example, a count of how many people listed “white” as their hair colour, how would we deal with this column?\nYou could do it in base R by enumerating all of the different strings and checking if the row contains any:\n\n# one moment I will give it up to python,\n# R does not have any base objects for dictionaries\n# or set-like collections\n\nmy_colours = list() \n\nfor(character in 1:nrow(sw)){\n  \n  current_colours = sw$skin_color[character]\n  current_colours = trimws(unlist(strsplit(current_colours,split = \",\")))\n  \n  my_colours = append(my_colours, current_colours)\n}\n\nmy_colours = unique(unlist(my_colours))\nmy_colours\n\n [1] \"fair\"          \"gold\"          \"white\"         \"blue\"         \n [5] \"light\"         \"red\"           \"unknown\"       \"green\"        \n [9] \"green-tan\"     \"brown\"         \"pale\"          \"metal\"        \n[13] \"dark\"          \"brown mottle\"  \"grey\"          \"mottled green\"\n[17] \"orange\"        \"yellow\"        \"tan\"           \"silver\"       \n[21] \"none\"         \n\n\nNow we have all of the possible values. To see which rows contain one of the values, we have to create an empty dataframe and iterate over the original:\n\n# create the dataframe\nsw_colours = data.frame(matrix(nrow = 0, ncol = length(my_colours)))\nnames(sw_colours) = my_colours\n\n# iterate over the original data frame counting hits for each\nfor(character in 1:nrow(sw)){\n  \n  hits = sapply(names(sw_colours), function(x) grepl(x, sw$skin_color[character]), USE.NAMES = FALSE)\n  sw_colours[character,] = hits\n}\n\nAnd there you have it, join these two by name:\n\nsw_colours$name = sw$name\nsw_with_checkboxes = merge(sw, sw_colours, by = \"name\")\nhead(sw_with_checkboxes[,c(\"name\", names(sw_colours))])\n\n                 name  fair  gold white  blue light   red unknown green\n1              Ackbar FALSE FALSE FALSE FALSE FALSE FALSE   FALSE FALSE\n2          Adi Gallia FALSE FALSE FALSE FALSE FALSE FALSE   FALSE FALSE\n3    Anakin Skywalker  TRUE FALSE FALSE FALSE FALSE FALSE   FALSE FALSE\n4        Arvel Crynyd  TRUE FALSE FALSE FALSE FALSE FALSE   FALSE FALSE\n5         Ayla Secura FALSE FALSE FALSE  TRUE FALSE FALSE   FALSE FALSE\n6 Bail Prestor Organa FALSE FALSE FALSE FALSE FALSE FALSE   FALSE FALSE\n  green-tan brown  pale metal  dark brown mottle  grey mottled green orange\n1     FALSE  TRUE FALSE FALSE FALSE         TRUE FALSE         FALSE  FALSE\n2     FALSE FALSE FALSE FALSE  TRUE        FALSE FALSE         FALSE  FALSE\n3     FALSE FALSE FALSE FALSE FALSE        FALSE FALSE         FALSE  FALSE\n4     FALSE FALSE FALSE FALSE FALSE        FALSE FALSE         FALSE  FALSE\n5     FALSE FALSE FALSE FALSE FALSE        FALSE FALSE         FALSE  FALSE\n6     FALSE FALSE FALSE FALSE FALSE        FALSE FALSE         FALSE  FALSE\n  yellow   tan silver  none              name.1\n1  FALSE FALSE  FALSE FALSE              Ackbar\n2  FALSE FALSE  FALSE FALSE          Adi Gallia\n3  FALSE FALSE  FALSE FALSE    Anakin Skywalker\n4  FALSE FALSE  FALSE FALSE        Arvel Crynyd\n5  FALSE FALSE  FALSE FALSE         Ayla Secura\n6  FALSE  TRUE  FALSE FALSE Bail Prestor Organa\n\n\nAdmittedly, this is all quite long-winded and could even have been done better. But fortunately, someone has already tackled this problem, and the solution is available on CRAN: The splitstackshape package.\nThis package has handy function concat.split() and variants for our case:\n\nsw_expanded=concat.split(sw, \"skin_color\", structure=\"expanded\", type=\"character\", fill = 0)\nhead(select(sw_expanded, name, matches(\"skin_color\")))\n\n            name  skin_color skin_color_blue skin_color_brown\n1 Luke Skywalker        fair               0                0\n2          C-3PO        gold               0                0\n3          R2-D2 white, blue               1                0\n4    Darth Vader       white               0                0\n5    Leia Organa       light               0                0\n6      Owen Lars       light               0                0\n  skin_color_brown mottle skin_color_dark skin_color_fair skin_color_gold\n1                       0               0               1               0\n2                       0               0               0               1\n3                       0               0               0               0\n4                       0               0               0               0\n5                       0               0               0               0\n6                       0               0               0               0\n  skin_color_green skin_color_green-tan skin_color_grey skin_color_light\n1                0                    0               0                0\n2                0                    0               0                0\n3                0                    0               0                0\n4                0                    0               0                0\n5                0                    0               0                1\n6                0                    0               0                1\n  skin_color_metal skin_color_mottled green skin_color_none skin_color_orange\n1                0                        0               0                 0\n2                0                        0               0                 0\n3                0                        0               0                 0\n4                0                        0               0                 0\n5                0                        0               0                 0\n6                0                        0               0                 0\n  skin_color_pale skin_color_red skin_color_silver skin_color_tan\n1               0              0                 0              0\n2               0              0                 0              0\n3               0              0                 0              0\n4               0              0                 0              0\n5               0              0                 0              0\n6               0              0                 0              0\n  skin_color_unknown skin_color_white skin_color_yellow\n1                  0                0                 0\n2                  0                0                 0\n3                  0                1                 0\n4                  0                1                 0\n5                  0                0                 0\n6                  0                0                 0\n\n\nEasy peasy!"
  },
  {
    "objectID": "posts/listing-legacy/2018-12-15-dplyr-crash-course.html",
    "href": "posts/listing-legacy/2018-12-15-dplyr-crash-course.html",
    "title": "Dplyr Crash Course",
    "section": "",
    "text": "Link to slides from the dplyr crash course I taught >>> LINK"
  },
  {
    "objectID": "posts/listing-legacy/2018-06-21-legacy-blog-posts.html",
    "href": "posts/listing-legacy/2018-06-21-legacy-blog-posts.html",
    "title": "Legacy Blog Posts",
    "section": "",
    "text": "That time I proposed that You’re Probably Already a Data Scientist. [link]\n\n\nThat time I did a fellowship with the Statistical And Mathematical Sciences Institute at NC State. [link]\n\n\nThat time I did analysis, visualisation, and topic modeling on the Gmail archive of Drexel University’s Peer Counseling Helpline. [link]\n\n\nThat time I interned at Salesforce. [link]\n\n\nThat time I took it a step further and built a dashboard app for the Peer Counseling Helpline to do data science themselves. [link]"
  },
  {
    "objectID": "posts/listing-legacy/2022-03-12-automated-cv-resume-with-r-circleci-google-sheets.html",
    "href": "posts/listing-legacy/2022-03-12-automated-cv-resume-with-r-circleci-google-sheets.html",
    "title": "Automated CV/Resume with R, CircleCI, & Google Sheets",
    "section": "",
    "text": "I hate updating my resume.\nA couple of years ago I was in a corporate tool limbo as I’d just left Drexel University, so my license with Microsoft Word had expired. In addition to having the most difficult time collaborating with MSW users on a paper, I also didn’t have a way to efficiently edit the resumes and CVs I’d developed. I tried opening files in Libre Office, which is a great freeware option for the office suite of tools, but frankly didn’t give seamless transition between features in MS."
  },
  {
    "objectID": "posts/listing-legacy/2022-03-12-automated-cv-resume-with-r-circleci-google-sheets.html#version-control-with-ms-word-sucks",
    "href": "posts/listing-legacy/2022-03-12-automated-cv-resume-with-r-circleci-google-sheets.html#version-control-with-ms-word-sucks",
    "title": "Automated CV/Resume with R, CircleCI, & Google Sheets",
    "section": "Version Control with MS Word Sucks",
    "text": "Version Control with MS Word Sucks\nIf you’ve ever created a document and named it myreport_ProfessorXfeedback_version4_March12_FINAL.docx, you already know what I mean.\n\nCredit: PhDcomics.com\nUsing plain text gives you the ability to version control files with Git+Github and similar tools, which gives you clearer commit history, change integration, etc. Now, admittedly, MS Word claims to have a version control system but I’ve never had much success with that, being totally honest, especially in comparison to Git-based workflows."
  },
  {
    "objectID": "posts/listing-legacy/2022-03-12-automated-cv-resume-with-r-circleci-google-sheets.html#formatting",
    "href": "posts/listing-legacy/2022-03-12-automated-cv-resume-with-r-circleci-google-sheets.html#formatting",
    "title": "Automated CV/Resume with R, CircleCI, & Google Sheets",
    "section": "Formatting",
    "text": "Formatting\nIn addition to version control sucking in MS word, managing reference format with something like bibtex also sucks; managing special characters from latex is a huge pain that isn’t handled well by MSW either. Again, plain text let’s you see things as you generate them, and worry about what they look like on a page later. Not to mention… tables 😤\n\n\n\nOnce again, you don’t have to worry about this formatting nonsense until you’re ready to “publish” your work, and when you are, you have the option to choose from a variety of formats that your content just gets dumped into. This website, for example,1 is built on the Goa template from the Hugo web framework. I just generate content in any plain text editor, and drop it into the framework that renders all the fancy bits for me. Speaking of which…"
  },
  {
    "objectID": "posts/listing-legacy/2022-03-12-automated-cv-resume-with-r-circleci-google-sheets.html#continuous-integration",
    "href": "posts/listing-legacy/2022-03-12-automated-cv-resume-with-r-circleci-google-sheets.html#continuous-integration",
    "title": "Automated CV/Resume with R, CircleCI, & Google Sheets",
    "section": "Continuous Integration",
    "text": "Continuous Integration\nThe best part of this project was making something that could be regenerated any time I needed it to, by an automated service. As opposed to having to open the Word document myself, make edits, save it, and export it, I have set up a system that does all of that for me at the push of a button. The only thing I need to do to make it work, is to edit the data that goes into the document. I do this with google sheets, but any data source can be used. This means when I earn a new title, publication, or role, I just open up google sheets, add a line to the doc with the details, then hit “Rerun” on the CircleCI continuous intergation service.\nThat’s it; that’s really it. No aligning tables, no content formatting, no shoddy print previews, and the output is accessible from CircleCI any time I need it."
  },
  {
    "objectID": "posts/listing-legacy/2022-03-12-automated-cv-resume-with-r-circleci-google-sheets.html#input-data-from-google-sheets",
    "href": "posts/listing-legacy/2022-03-12-automated-cv-resume-with-r-circleci-google-sheets.html#input-data-from-google-sheets",
    "title": "Automated CV/Resume with R, CircleCI, & Google Sheets",
    "section": "Input data from Google sheets",
    "text": "Input data from Google sheets\nThis is where it all starts; I found it great to organise it in a tidy format (each data point is a value, each column is a variable, each row is an observation). Furthermore, I can separate the data into different sheets for different CV categories."
  },
  {
    "objectID": "posts/listing-legacy/2022-03-12-automated-cv-resume-with-r-circleci-google-sheets.html#read-in-data-in-.rmd",
    "href": "posts/listing-legacy/2022-03-12-automated-cv-resume-with-r-circleci-google-sheets.html#read-in-data-in-.rmd",
    "title": "Automated CV/Resume with R, CircleCI, & Google Sheets",
    "section": "Read in Data In .Rmd",
    "text": "Read in Data In .Rmd\nI wrote a script in R that sets up and calls the Rmarkdown scripts, with configured parameters for both the long form CV and the one page resume. The RMarkdowns read in the data and parse it into a tidy format ready for the vitae and pagedown packages – these do most of the hard work."
  },
  {
    "objectID": "posts/listing-legacy/2022-03-12-automated-cv-resume-with-r-circleci-google-sheets.html#knit-with-css-latex",
    "href": "posts/listing-legacy/2022-03-12-automated-cv-resume-with-r-circleci-google-sheets.html#knit-with-css-latex",
    "title": "Automated CV/Resume with R, CircleCI, & Google Sheets",
    "section": "Knit with CSS + Latex",
    "text": "Knit with CSS + Latex\nThe packages for R CVs and resumes come with CSS definition files; I tinkered with these minimally, but I’m considering asking a CSS professional to make me a unique one. Once they’re ready to render, knitr takes it away, with some help from latex."
  },
  {
    "objectID": "posts/listing-legacy/2022-03-12-automated-cv-resume-with-r-circleci-google-sheets.html#circle-ci",
    "href": "posts/listing-legacy/2022-03-12-automated-cv-resume-with-r-circleci-google-sheets.html#circle-ci",
    "title": "Automated CV/Resume with R, CircleCI, & Google Sheets",
    "section": "Circle CI",
    "text": "Circle CI\nLastly, I created a Dockerfile that runs the above procedure on CircleCI. Once you upload the project to Github, and link that to Circle, then any time you add a commit to Github, the procedure runs as a job. You can configure how CircleCI runs in the config.yml, including storing the output files! These lines demonstrate how Circle knows to store outputs."
  },
  {
    "objectID": "posts/listing-legacy/listing-legacy.html",
    "href": "posts/listing-legacy/listing-legacy.html",
    "title": "Legacy Posts",
    "section": "",
    "text": "Automated CV/Resume with R, CircleCI, & Google Sheets\n\n\n\n\n\n\nTinashe M. Tapera\n\n\nMar 12, 2022\n\n\n\n\n\n\n\n\n\n\n\nData Tip: Assignment with the ’T’ Pipe\n\n\n\n\n\n\nTinashe M. Tapera\n\n\nJan 17, 2019\n\n\n\n\n\n\n\n\n\n\n\nData Tip: Processing Checkbox-Factors\n\n\n\n\n\n\nTinashe M. Tapera\n\n\nOct 23, 2018\n\n\n\n\n\n\n\n\n\n\n\nRemembering Mike Tapera Through Data Science & rtweet\n\n\n\n\n\n\nTinashe M. Tapera\n\n\nJun 21, 2018\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/listing-legacy/2019-01-17-data-tip-assignment-with-the-t-pipe.html",
    "href": "posts/listing-legacy/2019-01-17-data-tip-assignment-with-the-t-pipe.html",
    "title": "Data Tip: Assignment with the ’T’ Pipe",
    "section": "",
    "text": "# first, make some data, to write to disk\nmydat <- data(iris) \nfname <- \"iris.csv\"\nwrite.csv(x = mydat, fname)\n\n# do some coding\n# do some coding\n# do some coding\n\n# later on, reference the file using the fname variable\nfile.exists(fname)\n\n[1] TRUE\n\nfile.remove(fname)\n\n[1] TRUE\n\n\nFor some, this might be perfectly fine, but I found another interesting way of doing this using magrittr’s “Tee” pipe (%T>%):\n\nshhh <- suppressPackageStartupMessages\nshhh(library(dplyr, quietly = TRUE))\nshhh(library(magrittr, quietly = TRUE))\n\nfname2 <- \"iris2.csv\" %T>%\n  write.csv(mydat, .)\n\nfile.exists(fname2)\n\n[1] TRUE\n\nfile.remove(fname2)\n\n[1] TRUE\n\n\nJust an interesting way to assign an object and send the LHS of the pipe along the chain in one go."
  },
  {
    "objectID": "posts/listing-legacy/2018-06-21-getting-around-to-rtweet.html",
    "href": "posts/listing-legacy/2018-06-21-getting-around-to-rtweet.html",
    "title": "Remembering Mike Tapera Through Data Science & rtweet",
    "section": "",
    "text": "So recently my Twitter as been bloated by tweets left, right, and centre all to do with  and his work on the rtweet package. The hype train has certainly left the station, but I thought it was about time I checked out what all the fuss was about before I became completely out of touch, so let’s get down to it!\n\n\nTweets In Moratorium\nMy father Michael Tapera passed away from a brain tumour in 2016; although he will be sorely missed, it’s fortunate that my father was famous for his written and spoken word. He was an excellent writer and orator in just about every way, and a few days ago my mom suggested to our family to check his Twitter page for the 30 or so tweets he posted in his latter years (his diagnosis was received in 2013, and so all of these tweets were during his ailment).Of course, being the data scientist I am, simply reading the tweets would not suffice: I decided to do some text mining on my dad’s Twitter profile to find out what his online presence was like in his latter days.\n\n\nSetting Up\nLoading rtweet is really easy through CRAN, and setting up the Twitter API connection is similarly simple; see this page for a how-to.\n\n#install.packages(\"rtweet\")\nlibrary(rtweet)\nlibrary(tidyverse) #include for tidy R programming\nlibrary(plotly) #graphing\nlibrary(lubridate) #working with date times\nlibrary(tidytext) #text mining stuff\nlibrary(wordcloud) #a wordcloud, duh\nlibrary(knitr) #tables\nlibrary(kableExtra)\n\nNow that we’ve set up, we use rtweet’s handy functions to grab our data:\n\nauth_as(\"default\")\n\nReading auth from '/Users/tinashemtapera/Library/Preferences/org.R-project.R/R/\nrtweet/default.rds'\n\nSys.setenv(TZ=\"America_New_York\")\ntweets = get_timeline(c(\"MtaperaTapera\"))\n\nWarning in strptime(x, format, tz = tz): unknown timezone 'America_New_York'\n\n\n\n\nAnalysis and Visualisation\nWe can plot a general visualisation of dad’s Twitter activity.\n\ntweets%>%\n  select(created_at)%>%\n  ts_plot(\"days\")+\n  theme_minimal()+\n  labs(title=\"Mike Tapera's Twitter Timeline\", y=\"Number of Tweets\", x = \"Date\")\n\nWarning in format.POSIXlt(as.POSIXlt(x, tz), format, usetz, ...): unknown\ntimezone 'America_New_York'\n\nWarning in format.POSIXlt(as.POSIXlt(x, tz), format, usetz, ...): unknown\ntimezone 'America_New_York'\n\n\n\n\n\nIt looks like dad probably started tweeting around the time of his operation, and took a hiatus probably around his first surgery. His activity probably picked up again once he was back on his feet.\nWe can also see what day of the week or hour of day was most popular for his Twitter activity:\n\ntweets%>%\n  select(created_at)%>%\n  mutate(hour_of_day = hour(created_at),\n         day_of_week = wday(created_at, label = TRUE),\n         day_or_night = ifelse(hour_of_day > 11, \"pm\", \"am\"))%>%\n  mutate(hour_of_day = ifelse(hour_of_day < 1, 12,\n                              ifelse(hour_of_day > 12, hour_of_day-12, hour_of_day)))%>%\n  count(hour_of_day, day_of_week, day_or_night)%>%\n  ggplot(aes(x=hour_of_day, y=n))+\n  geom_bar(aes(fill = day_or_night),stat=\"identity\")+\n  facet_wrap(~day_of_week)+\n  coord_polar(start=0.25)+\n  theme_minimal()+\n  scale_x_continuous(breaks = 1:12,minor_breaks = NULL)+\n  theme(axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank(),\n        axis.title.x=element_blank(),\n        legend.position = \"left\",\n        legend.title = element_blank())+\n  labs(title=\"\\\"Clock Plots\\\" of Mike Tapera's\\n Twitter Activity\")\n\nWarning in format.POSIXlt(as.POSIXlt(x, tz), format, usetz, ...): unknown\ntimezone 'America_New_York'\n\n\n\n\n\nI call these clock plots — the length of the bar on the clock shows how many tweets dad sent during that hour of day, while the colour differentiates between AM or PM1. It’s interesting, though not unexpected, that dad used to tweet the most at 5AM on a Monday morning — members of the Tapera household are surely familiar with how dad used to get us up early to do Bible readings and devotions, or have some family time to encourage us. After my older brother and I left home, it’s not unusual that he started sharing his early morning/start of the week encouragements with the Twitterverse.\n\n\nWhat Did He Tweet About?\nOf course, it’d also be nice to get an overview of what dad used to Tweet about. We can do some simple text mining on his Twitter feed to find out, using the tidytext package.\n\n#create a dataframe to work with\ntext_df = data.frame(tweet = 1:nrow(tweets), date = date(tweets$created_at), text = tweets$text, stringsAsFactors = FALSE)\n\nWarning in as.POSIXlt.POSIXct(x, tz = tz(x)): unknown timezone\n'America_New_York'\n\n#tokenise\ntidy_tweets = text_df%>%\n  unnest_tokens(word, text)\n\n#remove stop words (functional words with no contextual importance)\ndata(stop_words)\ntidy_tweets = tidy_tweets %>%\n  anti_join(stop_words)\n\nNow let’s visualise what words he used most in his Twitter:\n\ntidy_tweets%>%\n  count(word)%>%\n  filter(n>1)%>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(word, n)) +\n  geom_col()+\n  coord_flip()+\n  theme_minimal()+\n  labs(title=\"Mike Tapera's Most Commonly\\nTweeted Words\")\n\n\n\n\nFor those who knew my dad, you can very clearly hear him mentioning a lot of these words quite often in formal conversation. My dad was not only a successful accountant, but he was also a pastor, family counselor, and public speaker. These words reflect those duties quite well.\n\n\nSentiment Analysis\nAnother interesting analysis is that of sentiment, which can tell us a general idea of the emotions within of a body of text. Thanks to the tidytext package, this is also relatively easy to do with dad’s tweets.\nThe NRC Lexicon is an extremely useful dataset in which the authors assigned a plethora of words to 1 (or more) of 8 fundamental human emotions: anger, fear, anticipation, trust, surprise, sadness, joy, and disgust. Using this lexicon, we can filter our Twitter tokens to find out which of these emotions dad tweeted about most (with some overlap, of course).\n\nnrc = get_sentiments(\"nrc\")\ntidy_tweets%>%\n  inner_join(nrc)%>%\n  filter(sentiment != \"negative\" & sentiment != \"positive\")%>%\n  count(sentiment, sort = TRUE)%>%\n  kable()%>%\n  kable_styling()\n\n\n\n \n  \n    sentiment \n    n \n  \n \n\n  \n    trust \n    21 \n  \n  \n    anticipation \n    20 \n  \n  \n    joy \n    20 \n  \n  \n    fear \n    12 \n  \n  \n    surprise \n    5 \n  \n  \n    anger \n    4 \n  \n  \n    sadness \n    4 \n  \n  \n    disgust \n    3 \n  \n\n\n\n\n\nWith some overlap, we can see that dad tweeted most with trust, anticipation, and joy words. Encouraging 😊, but some words can belong to different sentiment categories (e.g. “guard” is categorised under both fear, and trust). Instead, we can go with the positive/negative 5-scale score of the AFINN lexicon, to give us sentiment scores of each available word, and then average these scores for each tweet.\n\nafinn = get_sentiments(\"afinn\")\ntidy_tweets%>%\n  inner_join(afinn)%>%\n  group_by(tweet)%>%\n  mutate(score = value) %>%\n  mutate(sentiment = mean(score))%>%\n  select(-c(one_of(\"word\", \"score\")))%>%\n  distinct()%>%\n  ggplot(aes(x=tweet, y=sentiment))+\n  geom_bar(stat = \"identity\")+\n  theme_minimal()+\n  labs(title=\"Average Sentiment of Mike Tapera's\\nTweets Over Time\",\n       x = \"Tweet Number\",\n       y = \"Average Sentiment Score\")+\n  scale_x_continuous(limits = c(1,31), breaks = seq(0,30,5), minor_breaks = 1:31)\n\n\n\n\nIt’s great to see that on Twitter, dad was rarely negative and had positive things to say even towards the end of his life when facing the ominousness of brain cancer.\n\n\nObligatory Word Cloud\nNo Twitter text mining exercise is complete without a word cloud, although in my opinion they are often quite useless2.\n\ntidy_tweets%>%\n  count(word)%>%\n  with(., (wordcloud(word, n, max.words = 100,min.freq = 2)))\n\n\n\n\nNULL\n\n\n\n\nConclusion\nA few small takeaways are that we’re reminded how driven dad was by early mornings and motivating others at the start of the week. We also got to see what words he was using commonly online as well as the general sentiment of his tweets over time.\nDad: Although it was a tragedy to lose you, especially before I could graduate and show you all the skills and expertise I developed in university, I know you were always proud of me and that you loved me and our family very much. This is the first and most important thing I wanted to do with my time after graduation, and I hope it’s befitting. We love you and miss you dad.\n\n\n\n\n\nFootnotes\n\n\nIt’s important not to use polar coordinate plots in scientific settings, due to their tendency to be misperceived. See this source for more.\n\n↩︎\nWordclouds suck; see  this source for more.↩︎"
  },
  {
    "objectID": "posts/02_so-you-wanna-work-in-neuroscience/02_so-you-wanna-work-in-neuroscience.html",
    "href": "posts/02_so-you-wanna-work-in-neuroscience/02_so-you-wanna-work-in-neuroscience.html",
    "title": "So You Wanna Get Into Neuroscience…?",
    "section": "",
    "text": "After 4 eventful years, I left my position as Senior Neuroimaging Data Analyst at my lab, PennLINC, in August of 2022. And I’ll be honest — I didn’t know very much about neuroimaging when I began. I felt very much like Jen in the I.T. Crowd, having joined a circle of passionate geeks who could smell my impostor syndrome from a mile away.\n\n\n\n\n\nvia Gfycat\n\nAnd while I’m shifting away from neuroscience itself, I can confidently say I am proud to have had the opportunity to work in it. Neuroscience is an exciting, active, and rewarding field that is growing at lightning speed. Alongside the rise of data science as the “sexiest job of the 21st century”1, neuroscience has established itself as one of the flagship fields for data science-type work. A cohort of even a few dozen participants in a neuroscience experiment can generate hundreds of gigabytes of functional imaging data. The math, statistics, and machine learning methods are novel and complex, and being improved upon every day. And the cross-disciplinary nature means you can cater your research interests to include anything from cognitive science and philosophy of mind, to optogenetics and biomechanics. All in all, neuroscience is a field ripe for the budding data scientist of any kind, as a PhD candidate or staff scientist.\nSadly, though, neuroscience does have a critical flaw: it suffers from the ivory tower syndrome of academia. That is to say, there are many talented and intelligent people within neuroscience, but a lot of the skills required to get started in neuroscience are not talked about nearly as much as they could be. Furthermore, my lived experience is testament to the fact that a lot of these skills are far more accessible than we believe, and one does not need a PhD to get started on a neuroscience journey. While I believe that this is an epistemological shortcoming, I also believe it can be remedied with a little bit more transparency and openness. So, in this post I’m going to outline 4 technical skills that neuroscientists use every day, and hopefully convince you that you don’t need to be a PhD student to get started in neuroscience… with the help of I.T. Crowd gifs."
  },
  {
    "objectID": "posts/02_so-you-wanna-work-in-neuroscience/02_so-you-wanna-work-in-neuroscience.html#technical-skills-neuroscientists-use-every-day",
    "href": "posts/02_so-you-wanna-work-in-neuroscience/02_so-you-wanna-work-in-neuroscience.html#technical-skills-neuroscientists-use-every-day",
    "title": "So You Wanna Get Into Neuroscience…?",
    "section": "4 Technical Skills Neuroscientists Use Every Day",
    "text": "4 Technical Skills Neuroscientists Use Every Day\n\n1. The Command Line\nIf you’re going to work with big data, you’re going to have to learn to navigate a computer without a mouse. The reason for this is simple — for that much data, most institutions rely on Linux-based compute clusters, not desktops and laptops. They store, process, and analyse their data using tools that would be infinitely harder to use if they tried to develop friendly GUIs for them. In fact, at PennLINC, we actively try to veer students and staff away from programs that claim to provide a fun GUI for our work, because the GUI adds extra inflexible dependencies actually cost us more time in the long run.\nThat being said, the command line can be intimidating! Modern computing has made general use so easy, that more advanced tasks can seem quite daunting. Here are a few points to remember when jumping into the command line:\n\nYou probably won’t break it permanently. Remember that computer engineers have set up systems (such as the sudo argument) to make sure that everyday users don’t bork their entire computer in one go. Even if you do mess up something, it’s usually a matter of incorrect paths or conflicting packages — these problems can usually be solved by simply starting over (just burn it down). In the words of Roy, “have you tried turning it off and on again?”\n\n\n\n\n\n\nvia GIPHY\n\n\nYou get faster the more you practice. This can’t be stated enough — clicking into a folder and reading the content feels pretty seamless, but you will find the more you practice that “windows explorer” or “finder” is actually incredibly opaque and hides critical information you need, and extracting it takes ages longer than at the command line. So stick with it and be patient with yourself, you’ll see the benefits one function at a time.\n\nTo get started with familiarising yourself with the command line, I recommend two dual approaches:\n\nStructured learning via something like RyansTutorials.net, which will get you into a structured, directed learning path with someone who has some pedagogical clout and knows how to teach people; and\nUnstructured learning via, “getting your hands dirty”. If you find yourself opening a folder and twiddling around, ask yourself, “how would I do this without Finder/Explorer?” Google the question — you’ll be surprised at how many everyday computer operations are simply Linux commands with a fancy button.\n\n\n\n2. Python & R\nI really don’t have to give much information on why these are necessary skills in neuroscience — data science requires coding languages — but I will say — Python has a pretty marginal edge in popularity for neuroscience at the moment. I believe the reason is that older neuroscientists came from the world of Matlab, and so tend to operationalise the techniques in terms of matrices, which are very accessible in Python’s numpy package. That being said, R is still handy in neuroscience for scientific documentation/publishing and statistical analysis/machine learning, so it is to every neuroscientist’s advantage to learn both Python & R. Not to mention, the majority of neuroimaging pipelines2 are being developed with nipype, an awesome Python package for stringing together complex and dynamic pipelines for preprocessing and analyzing the data.\nGenerally, I’d advise getting as much programming knowledge as you possibly can. Even the most clinically focused, non-techy neuroscientist has to fit at least one model to their data, and doing this requires programming. Even if you consider yourself “not a math person” or “not into that tech stuff”, don’t disqualify yourself from simply knowing how to do it if you have to. After all, as scientists, we are all really standard nerds.\n\nPk5 It Crowd GIFfrom Pk5 GIFs\n\n\n\n\n3. git & Version Control\nVersion control is the idea of saving and maintaining software code as it evolves over time. Much like Dropbox or Google Drive have options to “revert back to X version” of your file, version control is “software-for-software” that helps you monitor, track, and combine (merge), versions of software code you write. It’s so critical in neuroscience that our group, PennLINC, often would verify the robustness of other group’s pipelines by looking at their version control trends — how frequently does this group make changes to their software? What kind of issues do they make changes in response to? Are their changes reactionary or are they planned and added to a periodic software update milestone?\nIt should be clear that writing code (Python or R) should go hand in hand with understanding that code should never suffer from document_my-edits_version-17_FINAL_AUG2022_Supervisor-feedback_v4.docx syndrome — version control is how programmers avoid that problem. A couple of notes about version control:\n\nThere is no project too small for version control. Adding a directory to git costs nothing, so do it with pretty much any folder with code in it. If your folder contains large proprietary/binary files (like large files created by a specific program), just add them to your .gitignore to make sure they’re not tracked by git.\ngit is a software for version control; Github is a company that hosts version control servers. Even though I personally almost exclusively use Github services, they don’t inherently deserve as much power as they have, so don’t hesitate to check out alternatives like Gitlab.\nTry out the Github CLI (gh cli); it’s actually pretty great!\nIf you’d like to track large proprietary/binary files, git annex is available, but I actually recommend a handy tool called datalad for handling both code text and large files. I strongly believe datalad is going to become a neuroimaging and neuroscience standard in the coming years — YOU HEARD IT HERE FIRST FOLKS!\nOpen-source makes the world a better place. By sharing your code, you provide other people with the opportunity to learn and grow, and perhaps develop their own solutions to problems they have, their community has, or even you have. Part of the goal of this blog is to open-source knowledge so that everyone can have the opportunity to learn from my shortcomings, mistakes and experiences. git (with companies like Github) makes that extremely easy, so share your code online (ethically and with permission from your supervisor) with the neuroimaging community so people can learn from your awesomeness — even if you believe your code sucks!\n\n\nIt Does You No Harm To Look A Little Foolish GIFfrom It Does You No Harm GIFs\n\n\n\n\n4. Applied Data Science in Neuroscience\nThis one is more abstract, but generally I think of this as the way that it all comes together — the stats, math, code, and the brain science. Getting into this is admittedly tricky, because it’s not quite clear what you should be looking for.\nMy answer? Hear it straight from the horse’s mouth.\nA simple Google search for “neuroscience with Python” yields a handful of great online workbooks, notebooks, and tutorials, published by labs as a shared “lab documentation”. I’ve not vetted them all, but from a cursory glance, they’re all worth a look over. I’d recommend, though, to make sure you check out Andy’s Brain Book to get acquainted with functional MRI if you’re totally new to it. The book walks you through a number of common neuroimaging analysis tools that you’ll surely interact with if you decide to jump into neuroscience.\nI’ll give this caveat, however. These workbooks that cater specifically to the content of fMRI and neuroscience are somewhat less important, because this is ideally the stuff you’d learn in class and on the job, so-to-speak. What’s more important is that you come into the job with the technical proficiency to not be slowed down by a new contextual task. In other words, it’s better that you know Python very well, so that when someone asks you to use numpy to perform affine transforms on a masked functional image, your bottleneck should be asking “what’s masking?”, and not, “what’s numpy?”"
  },
  {
    "objectID": "posts/02_so-you-wanna-work-in-neuroscience/02_so-you-wanna-work-in-neuroscience.html#conclusion",
    "href": "posts/02_so-you-wanna-work-in-neuroscience/02_so-you-wanna-work-in-neuroscience.html#conclusion",
    "title": "So You Wanna Get Into Neuroscience…?",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe It Crowd Moss GIFfrom The It Crowd GIFs\n\n\nHopefully this overview of proficiencies can help you get a head start in your journey in Neuroscience. Neuroscience is a great scientific field, and I’m going to miss it."
  },
  {
    "objectID": "news.html",
    "href": "news.html",
    "title": "News",
    "section": "",
    "text": "September 7th 2022\nStarted my PhD at Northeastern University\n\n\nAugust 15th 2022\nLeft the University of Pennsylvania"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m Tinashe",
    "section": "",
    "text": "This is the landing page."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m a PhD student in computer science at Northeastern University’s Khoury College of Computer Sciences. Specifically, I’m part of the Personal Health Informatics program."
  },
  {
    "objectID": "about.html#my-background",
    "href": "about.html#my-background",
    "title": "About Me",
    "section": "My Background",
    "text": "My Background\nHaving grown up in Zimbabwe, my greatest passion was writing, producing, and performing music. I spent most of my time in my home church and my high school’s music department. I was accepted to Drexel University in 2013 to study psychology, and in my junior year I joined the accelerated BSMS in Psychology program mentored by Fengqing Zhang of the Quantitative Psychology & Statisics Lab. There, I learned about statistical/machine learning, data mining, and computer programming in R & Python, and learned to apply these skills in a range of mental & behavioural health studies. In 2017, I interned at Salesforce as a data scientist in People Analytics, where I worked on NLP and text mining problems geared to improve employee success, and in 2018, I graduated from Drexel with my BSc & MSc in psychology. \nFrom 2018 to 2022, I worked as a neuroimaging data analyst at the Penn Lifespan Informatics and Neuroimaging Center, where I developed and used various software and programming tools to process, curate, and analyse neuroimaging data.\nI began my PhD at Northeastern University in September 2022."
  },
  {
    "objectID": "about.html#my-research",
    "href": "about.html#my-research",
    "title": "About Me",
    "section": "My Research",
    "text": "My Research\nExplain SIMBA"
  },
  {
    "objectID": "about.html#when-im-not-doing-research",
    "href": "about.html#when-im-not-doing-research",
    "title": "About Me",
    "section": "When I’m Not Doing Research…",
    "text": "When I’m Not Doing Research…\nFoo bar"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "You can access my CV here"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Tinashe M. Tapera’s Blog",
    "section": "",
    "text": "A second title\nsome content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLegacy Posts\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nanother listing\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nManaging Expected Loop Failures with Purrr\n\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2022\n\n\nTinashe M. Tapera\n\n\n\n\n\n\n  \n\n\n\n\nSo You Wanna Get Into Neuroscience…?\n\n\nLessons Learned From 4 Years As a Staff Scientist in a Neuroimaging Lab (Guided by The I.T. Crowd)\n\n\n\n\n\n\n\n\n\nAug 20, 2022\n\n\nTinashe M. Tapera\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest2\n\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2022\n\n\nTinashe M. Tapera\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest1\n\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2022\n\n\nTinashe M. Tapera\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated CV/Resume with R, CircleCI, & Google Sheets\n\n\n\n\n\n\n\ndata science\n\n\nrmarkdown\n\n\nR\n\n\ncontinuous integration\n\n\n\n\n\n\n\n\n\n\n\nMar 12, 2022\n\n\nTinashe M. Tapera\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Tip: Assignment with the ’T’ Pipe\n\n\n\n\n\n\n\ndplyr\n\n\nmagrittr\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2019\n\n\nTinashe M. Tapera\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDplyr Crash Course\n\n\n\n\n\n\n\ndplyr\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2018\n\n\nTinashe M. Tapera\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Tip: Processing Checkbox-Factors\n\n\n\n\n\n\n\ndplyr\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2018\n\n\nTinashe M. Tapera\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEncourageMe: A Shiny App for Encouragement\n\n\n\n\n\n\n\nR\n\n\ndata science\n\n\npsychology\n\n\nRShiny\n\n\nweb programming\n\n\n\n\n\n\n\n\n\n\n\nSep 22, 2018\n\n\nTinashe M. Tapera\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemembering Mike Tapera Through Data Science & rtweet\n\n\n\n\n\n\n\nR\n\n\nmachine learning\n\n\ndata science\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2018\n\n\nTinashe M. Tapera\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLegacy Blog Posts\n\n\n\n\n\n\n\ndata science\n\n\nR\n\n\nrmarkdown\n\n\nmachine learning\n\n\nold stuff\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2018\n\n\nTinashe M. Tapera\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/03_protect-purrr-loops/03_protect-purrr-loops.html",
    "href": "posts/03_protect-purrr-loops/03_protect-purrr-loops.html",
    "title": "Managing Expected Loop Failures with Purrr",
    "section": "",
    "text": "library(dplyr)\nlibrary(purrr)\n\nset.seed(12345)\nLooping is a fundamental programming paradigm. You have a set of inputs, and you wanna run a function on each of them:\nWith the purrr library, we get the same functionality as looping1 but with an arguably friendlier interface and more compliant mechanics with the idiosyncracies of the tidyverse:\nThis is all fine and dandy, but let’s say you get a failure from the data, like, add_ten throws an error if the output is greater than 100:\nIn a for loop, this fails as expected:\nIf I had to debug it this code, I would probably set up an iterator:\nIt failed at 5, so I’ll check inputs[5] and debug:\nBut with purrr::map(), there isn’t a straightforward way to debug like this. And if you have a large dataset, with a long-running function, you probably don’t want to wait until the map call fails and you have to go digging around into exactly which object in the vector had the problem.\nEnter: safely() and possibly().\nThese are two functions that modify the behaviour of a purrr call. You can wrap your function in one of these, and purrr will give you a couple of ways of managing what happens if and when your loop fails or throws some kind of warning or unexpected output. Here’s an example with the add_ten function, using quietly() to force map to keep going even if there’s a failure:\nHere, we see that safely() returns a list of outputs, with result and error. Implementing this in our dplyr chain would thus look like:\nWhat if we want to have a default value returned if there is an error? Well, in base R we’d do something like this:\nBut in purrr, safely() comes with the option to just specify this in the function with the otherwise argument! Check it out:\nThis is very useful! What’s more, the possibly() function defaults to only returning the successful result or the error condition, so you don’t even have to deal with a janky list output:\nWhich is easily parseable:"
  },
  {
    "objectID": "posts/03_protect-purrr-loops/03_protect-purrr-loops.html#why-is-this-useful",
    "href": "posts/03_protect-purrr-loops/03_protect-purrr-loops.html#why-is-this-useful",
    "title": "Managing Expected Loop Failures with Purrr",
    "section": "Why Is This Useful",
    "text": "Why Is This Useful\nI’d say this is a useful family of functions in a limited handful of scenarios, but comes in clutch when you meet them. When I first tried these functions out, I was processing a number of input files (n < 1000) with an external Matlab function that read in the file, calculated a parameter, and sent it back to R. In my experience, this approach was great because I 1) a long-ish list of inputs to a function, 2) had a function that took more than 5-10 minutes to run, per input, and 3) had an expected failure case that I didn’t much care about (parameter inputs were invalid) and predictable/not unexpected, so I didn’t quite want to handle them with a within-function tryCatch strategy.\nIn fact, most programmers (probably Python folks) are probably asking right now, “why would’t you just use a tryCatch and not deal with another dependency?”\nWell, the answer is that I think with this method, we keep the functions much more compact and straightforward, while also acknowledging that I will get errors returned when I expect them. This would be an unsafe approach when I do not know what inputs are expected, and what exactly can go wrong. But on this particular afternoon at work, I knew pretty much every input dataset, and knew/didn’t care about the reasons for a failure of the processing. I felt that this scenario lended itself well to the prima facie, handwavy approach of using otherwise in what’s essentially an apply call with syntactic sugar.\nSo, the lesson here is, use purrr functions instead of your loops. Or don’t, I guess. I’m not the expert here. I was honestly just tired and needed a better solution that “check each of these files for the different errors they could throw”, purrr worked out perfectly.\nAnyway, here’s a perfect loop to summarise this blog post. Any loop can be perfect, but when they are, they’re kinda freaky. Best to expect some failures.\nVideo"
  }
]